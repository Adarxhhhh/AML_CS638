{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c56b953e-172d-4683-8985-716074c32fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Conv2DTranspose, Flatten, Reshape, LeakyReLU, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4a8e9c8-c741-4771-b37f-a0c3611701ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, [64, 64])  # Resize images to 64x64\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize images to [0, 1]\n",
    "    return image\n",
    "\n",
    "#Loading the Dataset\n",
    "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
    "    'malaria',\n",
    "    split=['train[:70%]', 'train[70%:85%]', 'train[85%:]'],\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "ds_train = ds_train.map(preprocess).shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_val = ds_val.map(preprocess).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.map(preprocess).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80488085-db7b-4785-9cc7-b93231f8637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Generator and Discriminator\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(8*8*256, use_bias=False, input_shape=(100,)),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(),\n",
    "        tf.keras.layers.Reshape((8, 8, 256)),\n",
    "        Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(),\n",
    "        Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(),\n",
    "        Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(),\n",
    "        Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[64, 64, 3]),\n",
    "        LeakyReLU(),\n",
    "        Dropout(0.3),\n",
    "        Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "        LeakyReLU(),\n",
    "        Dropout(0.3),\n",
    "        Flatten(),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54c3c791-679a-49f9-a5d1-c4caf15111da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "#Defining Loss and Optimizers\n",
    "cross_entropy = BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "generator_optimizer = Adam(1e-4)\n",
    "discriminator_optimizer = Adam(1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a517f7fe-2198-42b0-b127-f7b6cce44be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Training and Validation Steps\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([batch_size, 100])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "@tf.function\n",
    "def val_step(images):\n",
    "    noise = tf.random.normal([batch_size, 100])\n",
    "    generated_images = generator(noise, training=False)\n",
    "    fake_output = discriminator(generated_images, training=False)\n",
    "    val_loss = discriminator_loss(tf.ones_like(fake_output), fake_output)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16cfaa13-84d2-4239-8c48-35b67a396011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.4832709729671478\n",
      "Epoch 2, Validation Loss: 0.9257936477661133\n",
      "Epoch 3, Validation Loss: 0.5708110332489014\n",
      "Epoch 4, Validation Loss: 0.38087767362594604\n",
      "Epoch 5, Validation Loss: 0.528873860836029\n",
      "Epoch 6, Validation Loss: 0.5845460891723633\n",
      "Epoch 7, Validation Loss: 0.48252153396606445\n",
      "Epoch 8, Validation Loss: 0.3405482769012451\n",
      "Epoch 9, Validation Loss: 0.6675868034362793\n",
      "Epoch 10, Validation Loss: 0.4260174036026001\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Execute the Training Loop\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        val_losses = []\n",
    "        for val_image_batch in ds_val:\n",
    "            loss = val_step(val_image_batch)\n",
    "            val_losses.append(loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Validation Loss: {tf.reduce_mean(val_losses).numpy()}\")\n",
    "\n",
    "# Step 7: Visualize Training Progress\n",
    "def plot_losses(val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Validation Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Initialize and train the GAN with validation\n",
    "epochs = 10\n",
    "train(ds_train, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbae522-1f76-4ef7-a91d-98a522a7e986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
